{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8052836-8264-4e5f-9d72-c14c6dc327dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================================================================================================#\n",
                "# Copyright 2023 Infosys Ltd.                                                                          #\n",
                "# Use of this source code is governed by Apache License Version 2.0 that can be found in the LICENSE file or at  #\n",
                "# http://www.apache.org/licenses/                                                                                #\n",
                "# ===============================================================================================================#"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2fe4da7-d98f-43f9-9d76-de115242d45f",
            "metadata": {},
            "source": [
                "## RAG Evaluation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "86f8ae5a-c014-4f16-886a-85b3739a7dd3",
            "metadata": {},
            "source": [
                "<div  style=\"line-height: 1;\">\n",
                "    <span style=\"color:Green\"> <b>PRE-REQUISITES : </b><br><br> \n",
                "        1.The library requires an <b>input config file</b> and <b>dataset file(s)</b> in order to perform the evaluation.<br>2.The input config file captures the configurations required for embeddings,llm,metrics used for evaluation and the path to the directory containing one or more dataset files.<br>3.The <b>ground_truth</b> needs to be provided along with <b>question</b> and a <b>context</b> in the dataset file.For evaluating a model , fetching answer is a capability provided by the library.<br>\n",
                "</span>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c99afeb-da74-448c-adbc-bb5630f17250",
            "metadata": {},
            "source": [
                "#### Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "493b386b-5d7f-4241-9400-8a27b8022908",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "import json\n",
                "from typing import List\n",
                "import pytest\n",
                "from langchain_openai import AzureOpenAI\n",
                "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
                "from langchain_openai.chat_models import AzureChatOpenAI\n",
                "\n",
                "import infy_fs_utils\n",
                "import infy_model_evaluation\n",
                "from infy_model_evaluation.common.constants import Constants\n",
                "from infy_model_evaluation.common.logger_factory import LoggerFactory\n",
                "from infy_model_evaluation.configuration import ClientConfigData\n",
                "from infy_model_evaluation.evaluator.process.rag_evaluator import RagEvaluator\n",
                "from infy_model_evaluation.data.config_data import EvaluatorMetrics, Result, TargetLlm, Datasource\n",
                "from infy_model_evaluation.data.config_data import EvaluatorConfigData\n",
                "from infy_model_evaluation.data.dataset import EvaluatorDataset\n",
                "from infy_model_evaluation.data.dataset import DatasetEntry"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcf99568-167d-44c0-9330-48b4eb8d7d88",
            "metadata": {},
            "source": [
                "#### Define configuration file path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "713cbdb2-87cf-47ad-b7f9-4c145f8d1332",
            "metadata": {},
            "outputs": [],
            "source": [
                "STORAGE_ROOT_PATH = 'C:/del/fs/notebookuc/STORAGE'\n",
                "CONTAINER_ROOT_PATH = 'C:/del/fs/notebookuc/CONTAINER'\n",
                "INPUT_CONFIG_FILE_PATH = '/data/config/input_config.json'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70f9a49c-a455-4248-992e-c67ed8d26177",
            "metadata": {},
            "source": [
                "#### Copying files\n",
                "<div style=\"line-height: 1;\">\n",
                "<span style=\"color:Red\"><b>NOTE: </b>In this notebook below is used to copy sample files to folders in <i>STORAGE_ROOT_PATH</i>.<br>\n",
                "In production the data and config files should be kept under respective folders in <i>STORAGE_ROOT_PATH </i>.<br>\n",
                "</span>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe17d208-636c-4d69-8408-9aec2b930580",
            "metadata": {},
            "outputs": [],
            "source": [
                "current_data_path = os.path.abspath('./data')\n",
                "\n",
                "if not os.path.exists(f'{STORAGE_ROOT_PATH}/data'):\n",
                "    os.makedirs(f'{STORAGE_ROOT_PATH}/data')\n",
                "if not os.path.exists(f'{STORAGE_ROOT_PATH}/data/input'):\n",
                "    os.makedirs(f'{STORAGE_ROOT_PATH}/data/input')     \n",
                "shutil.copy(f'{current_data_path}/dataset_file.json',\n",
                "            f'{STORAGE_ROOT_PATH}/data/input/dataset_file.json')\n",
                "shutil.copy(f'{current_data_path}/input_config.json',\n",
                "            f'{STORAGE_ROOT_PATH}/data/config/input_config.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1ff0e189-1291-4389-9f45-bfef6a16b1a2",
            "metadata": {
                "jp-MarkdownHeadingCollapsed": true,
                "tags": []
            },
            "source": [
                "#### Initialize Client Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "92806b8d-9f94-42e1-ad65-c3b8a3bd3080",
            "metadata": {},
            "outputs": [],
            "source": [
                "storage_config_data = infy_fs_utils.data.StorageConfigData(\n",
                "        **{\n",
                "            \"storage_root_uri\": f\"file://{STORAGE_ROOT_PATH}\",\n",
                "            \"storage_server_url\": \"\",\n",
                "            \"storage_access_key\": \"\",\n",
                "            \"storage_secret_key\": \"\"\n",
                "        })\n",
                "\n",
                "file_sys_handler = infy_fs_utils.provider.FileSystemHandler(\n",
                "    storage_config_data)\n",
                "if not infy_fs_utils.manager.FileSystemManager().has_fs_handler(\n",
                "    Constants.FSH_MODEL_EVALUATION):\n",
                "    infy_fs_utils.manager.FileSystemManager().set_root_handler_name(\n",
                "                    Constants.FSH_MODEL_EVALUATION)\n",
                "    infy_fs_utils.manager.FileSystemManager().add_fs_handler(file_sys_handler)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e5d8cf7-f585-4e8f-9f28-798889beebfa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure client properties\n",
                "client_config_data = ClientConfigData(\n",
                "        **{\n",
                "            \"container_data\": {\n",
                "                \"container_root_path\": f\"{CONTAINER_ROOT_PATH}\",\n",
                "            }\n",
                "        }\n",
                "    )\n",
                "infy_model_evaluation.ClientConfigManager().load(client_config_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c27cd994-cec2-45b8-9115-38c244b18768",
            "metadata": {
                "jp-MarkdownHeadingCollapsed": true,
                "tags": []
            },
            "source": [
                "#### Initialize Logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70059612-7df0-4fa0-9288-a740fdbd28e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "# Modify as required to control the overall logging level\n",
                "logging.basicConfig(level=logging.ERROR)\n",
                "logging_config_data = infy_fs_utils.data.LoggingConfigData(\n",
                "        **{\n",
                "            # \"logger_group_name\": \"my_group_1\",\n",
                "            \"logging_level\": 10,\n",
                "            \"logging_format\": \"\",\n",
                "            \"logging_timestamp_format\": \"\",\n",
                "            \"log_file_data\": {\n",
                "                \"log_dir_path\": \"/logs\",\n",
                "                \"log_file_name_prefix\": \"infy_model_evaluation\",\n",
                "                \"log_file_name_suffix\": \"\",\n",
                "                \"log_file_extension\": \".log\"\n",
                "\n",
                "            }})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcd4d442-e58e-4009-ad49-76d58fe4c114",
            "metadata": {},
            "outputs": [],
            "source": [
                "if not infy_fs_utils.manager.FileSystemLoggingManager().has_fs_logging_handler(\n",
                "    Constants.FSH_MODEL_EVALUATION):\n",
                "    file_sys_logging_handler = infy_fs_utils.provider.FileSystemLoggingHandler(\n",
                "                logging_config_data, file_sys_handler)\n",
                "    infy_fs_utils.manager.FileSystemLoggingManager(\n",
                "            ).set_root_handler_name(Constants.FSH_MODEL_EVALUATION)\n",
                "    infy_fs_utils.manager.FileSystemLoggingManager(\n",
                "            ).add_fs_logging_handler(file_sys_logging_handler)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff989b1b-4249-4c83-bcf4-c7a26942bdb2",
            "metadata": {},
            "source": [
                "### Build the Library Config"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7818edf-bafa-44e4-9356-803202435e15",
            "metadata": {},
            "source": [
                "<div  style=\"line-height: 1;\">\n",
                "    <span style=\"color:Red\"><b>WARNING:</b><br>\n",
                "        Before running the below code make sure to fill the values for <i>api_url</i> and  <i>api_key</i> fields in the <i>input_config.json</i> with the appropriate values.<br>Follow the steps below for details: <br>\n",
                "        <b>1: </b>Open the config file found in the following location <i>/data/config/input_config.json</i><br>\n",
                "        <b>2: </b>Inside the config update values for the above two fields for embedding and llm based on what type you want to use.<br>\n",
                "        <b></b>\n",
                "    <span>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "562a4574-a335-45a7-9bba-f38376db3a51",
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path = f'{INPUT_CONFIG_FILE_PATH}'\n",
                "config_file_content = file_sys_handler.read_file(file_path)\n",
                "request_config_data = json.loads(config_file_content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "478f72ee-1179-4c87-bbcb-8df297d4cbef",
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluator_config_data = request_config_data.get('evaluator', {})\n",
                "target_config_data = request_config_data.get('target', {})\n",
                "datasource_config_data = request_config_data.get('datasource', {})\n",
                "result_config_data = request_config_data.get('result', {})\n",
                "datasource_config = datasource_config_data.get('configuration')\n",
                "result_config = result_config_data.get('configuration')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6474f1af-c35b-4f0e-aa3a-e1191f327e42",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Evaluator Config\n",
                "for key, value in evaluator_config_data.items():\n",
                "        if key == 'embedding':\n",
                "            for key, val in value.items():\n",
                "                if key == \"openai\":\n",
                "                    if val.get('enabled'):\n",
                "                        embedding_config = val.get('configuration')\n",
                "                        break\n",
                "        if key == 'llm':\n",
                "            for key, val in value.items():\n",
                "                if key == \"openai\":\n",
                "                    if val.get('enabled'):\n",
                "                        llm_config = val.get('configuration')\n",
                "                        break\n",
                "        if key == 'metrics_list':\n",
                "            metrics = []\n",
                "            for metric in value:\n",
                "                if metric.get('enabled'):\n",
                "                    metrics.append(metric.get('name'))\n",
                "        if key == 'evaluation_only':\n",
                "            evaluation_only = value\n",
                "        if key == 'context_filter':\n",
                "            context_filter = value\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba797b31-50a0-48cf-9886-745d10ce098c",
            "metadata": {},
            "outputs": [],
            "source": [
                "for key, value in target_config_data.items():\n",
                "        if key == 'llm':\n",
                "            target_llm_config = value.get('configuration')\n",
                "            break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b2a4fc0-76b8-4597-accc-043805d6fc8e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare evaluator embedding config\n",
                "evaluator_embedding = AzureOpenAIEmbeddings(\n",
                "    **{\n",
                "        \"openai_api_type\": embedding_config.get('api_type'),\n",
                "        \"azure_endpoint\": os.environ['AZURE_OPENAI_SERVER_BASE_URL'],\n",
                "        \"api_key\": os.environ['AZURE_OPENAI_SECRET_KEY'],\n",
                "        \"openai_api_version\": embedding_config.get('api_version'),\n",
                "        \"model\": embedding_config.get('model_name'),\n",
                "        \"azure_deployment\": embedding_config.get('deployment_name'),\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b90e8c8-f171-4b4a-b99f-945914b0e721",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare evaluator llm config\n",
                "evaluator_llm_chat = None\n",
                "evaluator_llm = None    \n",
                "if (llm_config.get('is_chat_model')):\n",
                "        evaluator_llm_chat = AzureChatOpenAI(\n",
                "            **{\n",
                "                \"openai_api_type\": llm_config.get('api_type'),\n",
                "                \"azure_endpoint\": os.environ['AZURE_OPENAI_SERVER_BASE_URL'],\n",
                "                \"api_key\": os.environ['AZURE_OPENAI_SECRET_KEY'],\n",
                "                \"openai_api_version\": llm_config.get('api_version'),\n",
                "                \"model\": llm_config.get('model_name'),\n",
                "                \"azure_deployment\": llm_config.get('deployment_name'),\n",
                "            }\n",
                "        )\n",
                "else:\n",
                "    evaluator_llm = AzureOpenAI(\n",
                "            **{\n",
                "                \"openai_api_type\": llm_config.get('api_type'),\n",
                "                \"azure_endpoint\": os.environ['AZURE_OPENAI_SERVER_BASE_URL'],\n",
                "                \"api_key\": os.environ['AZURE_OPENAI_SECRET_KEY'],\n",
                "                \"openai_api_version\": llm_config.get('api_version'),\n",
                "                \"model\": llm_config.get('model_name'),\n",
                "                \"azure_deployment\": llm_config.get('deployment_name'),\n",
                "            }\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3a6ea0c0-347c-4955-8d36-7728a9acafff",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare evaluator metics config\n",
                "evaluator_metrics = EvaluatorMetrics(\n",
                "    **{\n",
                "        \"metrics\": metrics\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e53181c-d316-4c34-bacf-d55f1962cbf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare target llm config\n",
                "__target_llm = TargetLlm(**target_llm_config)\n",
                "__target_llm.api_key = os.environ['AZURE_OPENAI_SERVER_BASE_URL']\n",
                "__target_llm.api_url = os.environ['AZURE_OPENAI_SECRET_KEY']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "61564064-a354-4d41-82b5-7d1fe57b7b1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "__result = Result(**result_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "413c1d75-378b-41f3-aeed-3a72b5491622",
            "metadata": {},
            "outputs": [],
            "source": [
                "__datasource = Datasource(**datasource_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "973cf60b-d039-4508-a34c-be02da817a8d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare evaluator config data\n",
                "evaluator_config_data = EvaluatorConfigData(\n",
                "    embedding=evaluator_embedding,\n",
                "    llm=evaluator_llm,\n",
                "    llm_chat=evaluator_llm_chat,\n",
                "    metrics=evaluator_metrics.metrics,\n",
                "    target_llm=__target_llm,\n",
                "    evaluation_only=evaluation_only,\n",
                "    context_filter=context_filter,    \n",
                "    result=__result,\n",
                "    datasource=__datasource,\n",
                "    is_evaluator_llm_chat_model=llm_config.get('is_chat_model'),\n",
                "    evaluator_embedding_tiktoken_cache_dir=embedding_config.get(\n",
                "                'tiktoken_cache_dir')\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3779cbc5-e5ea-46cb-9c7e-9d640ba89910",
            "metadata": {},
            "source": [
                "<div style=\"line-height: 1;\">\n",
                "    <span ><b>NOTE: </b><br>Run the cell below <b>only</b> in case you want to evaluate a custom model<br><br>Configure the <i>api_url</i> and other parameters accordingly\n",
                "</span>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e98805f-6006-4b70-bbea-e19ffefc5d64",
            "metadata": {},
            "outputs": [],
            "source": [
                "# evaluator_config_data.target_llm.api_type= \"\"\n",
                "# evaluator_config_data.target_llm.api_url = os.environ['CUSTOM_LLM_URL']\n",
                "# evaluator_config_data.target_llm.max_tokens = 1024\n",
                "# evaluator_config_data.target_llm.temperature = 0.7\n",
                "# evaluator_config_data.target_llm.tiktoken_cache_dir = \"\"\n",
                "# evaluator_config_data.target_llm.remove_prompt_from_response = False\n",
                "# evaluator_config_data.target_llm.requires_num_return_sequences = False\n",
                "# evaluator_config_data.target_llm.num_return_sequences = 1\n",
                "# evaluator_config_data.target_llm.do_sample = True\n",
                "# evaluator_config_data.result.file_path = \"/evaluation_result_mixtral8x7b-instruct.json\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9df6dc10-5082-4e51-a769-e952238c2cfc",
            "metadata": {},
            "source": [
                "### Run the evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4528601-0b23-4129-af62-bfcc3a67f9b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluator = RagEvaluator()\n",
                "result = evaluator.evaluate(evaluator_config_data,[])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b8a9cd5-2985-486f-a3e7-543160a92f8c",
            "metadata": {},
            "source": [
                "### Verify results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "649c5cd9-ee4e-45ca-85ca-0833a89d431d",
            "metadata": {},
            "source": [
                "<div  style=\"line-height: 1;\">\n",
                "    <span style=\"color:Green\"><b>NOTE: </b> The results of the evaluation will be available in <i>evaluation_result.json</i> file at <i>STORAGE_ROOT_PATH</i>.</span></div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49348947-eb90-4820-a580-8c07068a75d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(result.get('aggregation'))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "65d6c42f-adbe-4b20-bda5-91fb6fae5f5f",
            "metadata": {},
            "source": [
                "### Further Processing"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "106f58cd-d0cd-45bb-8eec-4a148d643b94",
            "metadata": {},
            "source": [
                "<div  style=\"line-height: 1;\">\n",
                "    <span style=\"color:Green\"><b>NOTE: <br></b>This completes the model evaluation for the dataset file(s).<br>As a continuation step to visualise the metrics, refer <i>rag_metrics</i> under <b>reporter</b> which will require the <i>evaluation_result.json</i> file created above."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}