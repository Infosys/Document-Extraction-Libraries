{
  "name": "pipeline2",
  "description": "Sample 4-processor-pipeline configuration for integration testing",
  "variables": {
    "DPP_STORAGE_ROOT_URI": "file://D:/STORAGE",
    "DPP_STORAGE_SERVER_URL": "",
    "DPP_STORAGE_ACCESS_KEY": "",
    "DPP_STORAGE_SECRET_KEY": "",
    "OPENAI_KEY": "${ENV:OPENAI_KEY}",
    "OPENAI_SERVER_URL": "${ENV:OPENAI_SERVER_URL}",
    "MODEL_HOME": "C:/MyProgramFiles/AI/models",
    "CUSTOM_LLM_BLOOM_INFERENCE_URL": "${ENV:CUSTOM_LLM_BLOOM_INFERENCE_URL}",
    "CUSTOM_LLM_LLAMA_INFERENCE_URL": "${ENV:CUSTOM_LLM_LLAMA_INFERENCE_URL}",
    "CUSTOM_LLM_MIXTRAL_INFERENCE_URL": "${ENV:CUSTOM_LLM_MIXTRAL_INFERENCE_URL}"
  },
  "processor_list": [
    {
      "enabled": true,
      "processor_name": "retriever",
      "processor_namespace": "infy_dpp_ai.retriever.process.query_retriever_processor",
      "processor_class_name": "QueryRetriever",
      "processor_input_config_name_list": ["QueryRetriever"]
    },
    {
      "enabled": true,
      "processor_name": "reader",
      "processor_namespace": "infy_dpp_ai.reader.process.reader_processor",
      "processor_class_name": "Reader",
      "processor_input_config_name_list": ["Reader"]
    }
  ],
  "processor_input_config": {
    "QueryRetriever": {
      "embedding": {
        "openai": {
          "enabled": false,
          "configuration": {
            "api_type": "azure",
            "api_version": "2022-12-01",
            "api_url": "${OPENAI_SERVER_URL}",
            "api_key": "${OPENAI_KEY}",
            "model_name": "text-embedding-ada-002",
            "deployment_name": "text-embedding-ada-002",
            "chunk_size": 1000,
            "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding"
          }
        },
        "sentence_transformer": {
          "enabled": true,
          "configuration": {
            "model_name": "all-MiniLM-L6-v2",
            "model_home_path": "${MODEL_HOME}"
          }
        }
      },
      "storage": {
        "faiss": {
          "enabled": true,
          "configuration": {
            "chunked_files_root_path": "/data/vectordb/chunked",
            "encoded_files_root_path": "/data/vectordb/encoded",
            "db_name": "documents"
          }
        }
      },
      "queries": [
        {
          "attribute_key": "query1",
          "question": "What is the percentage of women employees?",
          "top_k": 2,
          "pre_filter_fetch_k": 10,
          "filter_metadata": {}
        }
      ]
    },
    "Reader": {
      "storage": {
        "faiss": {
          "enabled": true,
          "configuration": {
            "chunked_files_root_path": "/data/vectordb/chunked",
            "encoded_files_root_path": "/data/vectordb/encoded",
            "db_name": "documents"
          }
        }
      },
      "llm": {
        "openai": {
          "enabled": false,
          "configuration": {
            "api_type": "azure",
            "api_version": "2022-12-01",
            "api_url": "${OPENAI_SERVER_URL}",
            "api_key": "${OPENAI_KEY}",
            "max_tokens": 1000,
            "model_name": "text-davinci-003",
            "deployment_name": "text-davinci-003",
            "temperature": 0.7,
            "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding"
          },
          "cache": {
            "enabled": true,
            "cache_root_path": "/data/cache/infy_model_service"
          }
        },
        "custom": {
          "bloom-7b1": {
            "enabled": false,
            "configuration": {
              "inference_url": "${CUSTOM_LLM_BLOOM_INFERENCE_URL}",
              "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding",
              "remove_query_from_response": true
            },
            "json_payload": {
              "inputs": "{query}",
              "parameters": {
                "max_new_tokens": 4096,
                "temperature": 1,
                "num_return_sequences": 1,
                "do_sample": true
              }
            }
          },
          "llama2-7b": {
            "enabled": false,
            "configuration": {
              "inference_url": "${CUSTOM_LLM_LLAMA_INFERENCE_URL}",
              "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding",
              "remove_query_from_response": true
            },
            "json_payload": {
              "inputs": "{query}",
              "parameters": {
                "max_new_tokens": 4096,
                "temperature": 1,
                "num_return_sequences": 1,
                "do_sample": true
              }
            }
          },
          "mixtral8x7b-instruct": {
            "enabled": true,
            "configuration": {
              "inference_url": "${CUSTOM_LLM_MIXTRAL_INFERENCE_URL}",
              "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding",
              "remove_query_from_response": false
            },
            "json_payload": {
              "inputs": "{query}",
              "parameters": {
                "max_new_tokens": 1024,
                "temperature": 0.1
              }
            }
          }
        }
      },
      "named_context_templates": {
        "context_default": "{chunk_text}",
        "context_1": "[chunk_id={chunk_id},page_no={page_no},sequence_no={sequence_no},bbox={bbox},doc_name={doc_name}]\n{chunk_text}\n",
        "context_2": "[chunk_id={chunk_id},page_no={page_no},sequence_no={sequence_no}]\n{chunk_text}\n"
      },
      "named_prompt_templates": {
        "prompt_template_1": {
          "content": [
            "Use the following pieces of context to answer the question at the end.",
            "If you don't know the answer or even doubtful a bit, just say that you don't know,",
            " don't try to make up an answer.Just give the shortest and most appropriate relavant answer to the question.",
            "{context}",
            "Question: {question}",
            "Helpful Answer:"
          ],
          "context_template": "context_default"
        },
        "extractor_attribute_prompt": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt.txt",
          "context_template": "context_1"
        },
        "extractor_attribute_prompt_2": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt_2.txt",
          "context_template": "context_2"
        },
        "extractor_attribute_prompt_3": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt_3.txt",
          "context_template": "context_2"
        }
      },
      "inputs": [
        {
          "attribute_key": "query1",
          "prompt_template": "extractor_attribute_prompt_3",
          "model_type": "QnA"
        }
      ]
    }
  }
}
