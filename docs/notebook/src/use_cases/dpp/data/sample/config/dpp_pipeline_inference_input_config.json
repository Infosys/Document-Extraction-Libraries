{
  "name": "Batch inference pipeline",
  "description": "DPP batch inferencing pipeline",
  "variables": {
    "MODEL_HOME": "C:/del/ai/models",
    "AI_HOME": "C:/del/ai",
    "CA_CERTS_PATH": "",
    "AZURE_OPENAI_SERVER_BASE_URL": "${ENV:AZURE_OPENAI_SERVER_BASE_URL}",
    "AZURE_OPENAI_SECRET_KEY": "${ENV:AZURE_OPENAI_SECRET_KEY}",
    "LITELLM_PROXY_SERVER_BASE_URL": "${ENV:LITELLM_PROXY_SERVER_BASE_URL}",
    "LITELLM_PROXY_SECRET_KEY": "${ENV:LITELLM_PROXY_SECRET_KEY}",
    "INFY_DB_SERVICE_BASE_URL": "${ENV:INFY_DB_SERVICE_BASE_URL}",
    "INFY_MODEL_SERVICE_BASE_URL": "${ENV:INFY_MODEL_SERVICE_BASE_URL}"
  },
  "processor_list": [
    {
      "enabled": true,
      "processor_name": "request_creator",
      "processor_namespace": "infy_dpp_core.request_creator",
      "processor_class_name": "RequestCreator",
      "processor_input_config_name_list": [
        "RequestCreator"
      ]
    },
    {
      "enabled": true,
      "processor_name": "retriever",
      "processor_namespace": "infy_dpp_ai.retriever.process.query_retriever_processor",
      "processor_class_name": "QueryRetriever",
      "processor_input_config_name_list": [
        "QueryRetriever"
      ]
    },
    {
      "enabled": true,
      "processor_name": "reader",
      "processor_namespace": "infy_dpp_ai.reader.process.reader_processor",
      "processor_class_name": "Reader",
      "processor_input_config_name_list": [
        "Reader"
      ]
    },
    {
      "enabled": true,
      "processor_name": "request_closer",
      "processor_namespace": "infy_dpp_core.request_closer",
      "processor_class_name": "RequestCloser",
      "processor_input_config_name_list": [
        "RequestCloser"
      ]
    }
  ],
  "processor_input_config": {
    "RequestCreator": {
      "from_data_file": {
        "enabled": false,
        "read_path": "/data/input/",
        "batch_size": 20,
        "filter": {
          "include": [
            "pdf"
          ],
          "exclude": [
            "_"
          ]
        },
        "work_root_path": "/data/work/",
        "to_request_file": {
          "save_path": "/data/work/request/indexer/start"
        }
      },
      "from_request_file": {
        "enabled": true,
        "read_path": "/data/work/request/indexer/complete",
        "save_path": "/data/work/request/inference/start"
      }
    },
    "QueryRetriever": {
      "embedding": {
        "openai": {
          "enabled": false,
          "configuration": {
            "api_type": "azure",
            "api_version": "2022-12-01",
            "api_url": "${AZURE_OPENAI_SERVER_BASE_URL}",
            "api_key": "${AZURE_OPENAI_SECRET_KEY}",
            "model_name": "text-embedding-ada-002",
            "deployment_name": "text-embedding-ada-002",
            "chunk_size": 1000,
            "tiktoken_cache_dir": "${MODEL_HOME}/tiktoken_encoding"
          }
        },
        "sentence_transformer": {
          "enabled": true,
          "configuration": {
            "model_name": "all-MiniLM-L6-v2",
            "api_url": "${INFY_MODEL_SERVICE_BASE_URL}"
          }
        },
        "custom": {
          "enabled": false,
          "configuration": {
            "model_name": "mistral-embd",
            "api_key": "",
            "endpoint": "${CUSTOM_EMB_MISTRAL_INFERENCE_URL}"
          }
        }
      },
      "storage": {
        "vectordb": {
          "faiss": {
            "enabled": true,
            "configuration": {
              "chunked_files_root_path": "/data/vectordb/chunked",
              "encoded_files_root_path": "/data/vectordb/encoded",
              "db_name": "documents",
              "index_id": "",
              "collections": [
                {
                  "collection_name": "documents",
                  "collection_secret_key": ""
                }
              ],
              "distance_metric": {
                "eucledian": true
              }
            }
          },
          "infy_db_service": {
            "enabled": false,
            "configuration": {
              "db_service_url": "${INFY_DB_SERVICE_BASE_URL}",
              "model_name": "",
              "index_id": "",
              "collections": [
                {
                  "collection_name": "",
                  "collection_secret_key": ""
                }
              ]
            }
          },
          "elasticsearch": {
            "enabled": false,
            "configuration": {
              "db_server_url": "",
              "authenticate": "false",
              "username": "",
              "password": "",
              "verify_certs": "false",
              "cert_fingerprint": "",
              "ca_certs_path": "${CA_CERTS_PATH}",
              "index_id": ""
            }
          }
        },
        "sparseindex": {
          "bm25s": {
            "enabled": false,
            "configuration": {
              "sparse_index_root_path": "/data/db/sparseindex",
              "db_name": "documents",
              "index_id": "",
              "collections": [
                {
                  "collection_name": "",
                  "collection_secret_key": "",
                  "chunk_type": ""
                }
              ],
              "nltk_data_dir": "${AI_HOME}/nltk_data"
            }
          },
          "infy_db_service": {
            "enabled": false,
            "configuration": {
              "db_service_url": "${INFY_DB_SERVICE_BASE_URL}",
              "method_name": "",
              "index_id": "",
              "collections": [
                {
                  "collection_name": "",
                  "collection_secret_key": ""
                }
              ]
            }
          }
        }
      },
      "hybrid_search": {
        "rrf": {
          "enabled": false
        }
      },
      "queries": [
        {
          "attribute_key": "women_emp_pct",
          "question": "What is the percentage of women employees in Infosys?",
          "top_k": 1,
          "pre_filter_fetch_k": 10,
          "filter_metadata": {},
          "min_distance": 0,
          "max_distance": 4
        },
        {
          "attribute_key": "financial_year",
          "question": "What is the financial year of this report?",
          "top_k": 1,
          "pre_filter_fetch_k": 10,
          "filter_metadata": {},
          "min_distance": 0,
          "max_distance": 4
        },
        {
          "attribute_key": "revenue_amt",
          "question": "What is the Revenue amount of this year?",
          "top_k": 1,
          "pre_filter_fetch_k": 10,
          "filter_metadata": {},
          "min_distance": 0,
          "max_distance": 4
        },
        {
          "attribute_key": "business_rank",
          "question": "What is the rank of infosys?",
          "top_k": 1,
          "pre_filter_fetch_k": 10,
          "filter_metadata": {},
          "min_distance": 0,
          "max_distance": 4
        }
      ]
    },
    "Reader": {
      "storage": {
        "vectordb": {
          "faiss": {
            "enabled": true
          }
        },
        "sparseindex": {
          "bm25s": {
            "enabled": false
          }
        }
      },
      "hybrid_search": {
        "rrf": {
          "enabled": false
        }
      },
      "llm": {
        "models": [
          {
            "name": "Gpt-4-direct",
            "enabled": true,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${AZURE_OPENAI_SERVER_BASE_URL}",
              "api_key": "${AZURE_OPENAI_SECRET_KEY}",
              "model_name": "openai/gpt-4",
              "deployment_name": "gpt4",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          },
          {
            "name": "Gpt-35-turbo-direct",
            "enabled": false,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${AZURE_OPENAI_SERVER_BASE_URL}",
              "api_key": "${AZURE_OPENAI_SECRET_KEY}",
              "model_name": "openai/gpt-35-turbo",
              "deployment_name": "gpt-35-turbo",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          },
          {
            "name": "Gpt-4-proxy",
            "enabled": false,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${LITELLM_PROXY_SERVER_BASE_URL}",
              "api_key": "${AZURE_OPENAI_SECRET_KEY}",
              "model_name": "gpt-4-32k_2",
              "deployment_name": "gpt-4-32k_2",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          },
          {
            "name": "Llama-3.1-8B-Proxy",
            "enabled": false,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${LITELLM_PROXY_SERVER_BASE_URL}",
              "api_key": "${LITELLM_PROXY_SECRET_KEY}",
              "model_name": "Meta-Llama-3.1-8B-Instruct",
              "deployment_name": "Meta-Llama-3.1-8B-Instruct",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          },
          {
            "name": "Llama-3.1-70B-Proxy",
            "enabled": false,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${LITELLM_PROXY_SERVER_BASE_URL}",
              "api_key": "${LITELLM_PROXY_SECRET_KEY}",
              "model_name": "Meta-Llama-3.1-70B-Instruct-FP8",
              "deployment_name": "Meta-Llama-3.1-70B-Instruct-FP8",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          },
          {
            "name": "Mixtral-7B-Proxy",
            "enabled": false,
            "batch": {
              "size": 1
            },
            "configuration": {
              "api_url": "${LITELLM_PROXY_SERVER_BASE_URL}",
              "api_key": "${LITELLM_PROXY_SECRET_KEY}",
              "model_name": "mixtral-8x7b-instruct",
              "deployment_name": "mixtral-8x7b-instruct",
              "max_tokens": 1000,
              "temperature": 0.7
            }
          }
        ]
      },
      "named_context_templates": {
        "context_default": "{chunk_text}",
        "context_1": "[chunk_id={chunk_id},page_no={page_no},sequence_no={sequence_no},bbox={bbox},doc_name={doc_name}]\n{chunk_text}\n",
        "context_2": "[chunk_id={chunk_id},page_no={page_no},sequence_no={sequence_no}]\n{chunk_text}\n"
      },
      "named_prompt_templates": {
        "prompt_template_1": {
          "content": [
            "Use the following pieces of context to answer the question at the end.",
            "If you don't know the answer or even doubtful a bit, just say that you don't know,",
            " don't try to make up an answer.Just give the shortest and most appropriate relavant answer to the question.",
            "{context}",
            "Question: {question}",
            "Helpful Answer:"
          ],
          "context_template": "context_default"
        },
        "extractor_attribute_prompt": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt.txt",
          "context_template": "context_1"
        },
        "extractor_attribute_prompt_2": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt_2.txt",
          "context_template": "context_2",
          "response_validation": {
            "enabled": true,
            "type": "json",
            "total_attempts": 3
          }
        },
        "extractor_attribute_prompt_3": {
          "content": [],
          "file_path": "/data/config/prompt_templates/extractor_attribute_prompt_3.txt",
          "context_template": "context_2",
          "response_validation": {
            "enabled": true,
            "type": "json",
            "total_attempts": 3
          }
        }
      },
      "inputs": [
        {
          "attribute_key": "women_emp_pct",
          "prompt_template": "extractor_attribute_prompt_2",
          "model_type": "QnA",
          "top_k": 2
        },
        {
          "attribute_key": "financial_year",
          "prompt_template": "extractor_attribute_prompt_2",
          "model_type": "QnA",
          "top_k": 2
        },
        {
          "attribute_key": "revenue_amt",
          "prompt_template": "extractor_attribute_prompt_2",
          "model_type": "QnA",
          "top_k": 2
        },
        {
          "attribute_key": "business_rank",
          "prompt_template": "extractor_attribute_prompt_2",
          "model_type": "QnA",
          "top_k": 2
        }
      ]
    },
    "RequestCloser": {
      "work_root_path": "/data/work/",
      "data_file": {
        "output_root_path": ""
      },
      "output_root_path": "/data/output/",
      "from_request_file": {
        "read_path": "/data/work/request/inference/start",
        "save_path": "/data/work/request/inference/complete"
      }
    }
  }
}