{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a813a11a-fd97-4371-8d23-f81dca533c9a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================================================================================================#\n",
                "# Copyright 2024 Infosys Ltd.                                                                          #\n",
                "# Use of this source code is governed by Apache License Version 2.0 that can be found in the LICENSE file or at  #\n",
                "# http://www.apache.org/licenses/                                                                                #\n",
                "# ===============================================================================================================#"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2fe4da7-d98f-43f9-9d76-de115242d45f",
            "metadata": {},
            "source": [
                "## Tool 03 - Prompt Engineering (Interactive)\n",
                "To test different prompts on LLM"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c99afeb-da74-448c-adbc-bb5630f17250",
            "metadata": {},
            "source": [
                "#### Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "493b386b-5d7f-4241-9400-8a27b8022908",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "import re\n",
                "import json\n",
                "import tiktoken\n",
                "from IPython.display import display, HTML, Markdown\n",
                "from _internal_utils.q_n_a_visualizer import QnAVisualizer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7bd037f-2401-4fdd-9b95-cf384bed0340",
            "metadata": {},
            "source": [
                "#### Set environment variables\n",
                "<div  style=\"line-height: 1;\">\n",
                "    <span style=\"color:Red\"><b>NOTE:</b> The Pipeline uses environment variables which needs to be set by the developer.<br>\n",
                "In production developer needs to set them as required.<br>\n",
                "In this notebook you can provide them using the below code.<br>\n",
                "To set or change the value please refer <i>installation.ipynb</i></span>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a958432f-e9d2-40e5-9e9a-69f9072b165e",
            "metadata": {},
            "outputs": [],
            "source": [
                "%store -r CUSTOM_LLM_MIXTRAL_INFERENCE_URL\n",
                "os.environ['CUSTOM_LLM_MIXTRAL_INFERENCE_URL']=CUSTOM_LLM_MIXTRAL_INFERENCE_URL\n",
                "\n",
                "%store -r OPENAI_KEY\n",
                "os.environ['OPENAI_KEY'] = OPENAI_KEY\n",
                "%store -r OPENAI_SERVER_URL\n",
                "os.environ['OPENAI_SERVER_URL']=OPENAI_SERVER_URL\n",
                "\n",
                "os.environ[\"TIKTOKEN_CACHE_DIR\"] = r\"C:\\MyProgramFiles\\AI\\models\\tiktoken_encoding\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "96639c6b-ea06-4455-a9ce-d0020ae41988",
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_tokens(text):\n",
                "    encoding = tiktoken.get_encoding(\"p50k_base\")\n",
                "    count = len(encoding.encode(text))\n",
                "    return count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "099bfa66-f11a-40fd-91ad-2f9d04e4d570",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LlmService():\n",
                "\n",
                "    def call_mixtral(self, question, remove_query=False):\n",
                "        url = os.environ['CUSTOM_LLM_MIXTRAL_INFERENCE_URL']\n",
                "        response = self.__invoke_api(url, question)\n",
                "        if remove_query:\n",
                "            response = self.__remove_query(response, question)\n",
                "        return response\n",
                "\n",
                "    def call_open_ai(self, question):\n",
                "        url = os.environ['OPENAI_SERVER_URL']\n",
                "        url+= \"/openai/deployments/gpt4/chat/completions?api-version=2024-02-15-preview\"\n",
                "        headers = {\n",
                "            \"Content-Type\": \"application/json\",\n",
                "            \"api-key\": os.environ['OPENAI_KEY']\n",
                "        }\n",
                "#         data = {\n",
                "#             \"prompt\": question,\n",
                "#             \"max_tokens\": 100\n",
                "#         }\n",
                "        data={\n",
                "          \"messages\": [\n",
                "            {\n",
                "              \"role\": \"system\",\n",
                "              \"content\": \"You are a helpful assistant.\"\n",
                "            },\n",
                "            {\n",
                "              \"role\": \"user\",\n",
                "              \"content\": question\n",
                "            }\n",
                "          ],\n",
                "          \"max_tokens\": 2000,\n",
                "          \"stream\": False\n",
                "        }\n",
                "        response = requests.post(url, headers=headers,\n",
                "                                 data=json.dumps(data), timeout=180)\n",
                "        print(\"response\",response)\n",
                "        print(\"response.json()\",response.json())\n",
                "        print(\"response.json()['choices'][0]['message']\",response.json()['choices'][0]['message'])\n",
                "        return response.json()['choices'][0]['message'][\"content\"]\n",
                "\n",
                "    ## Private ##\n",
                "\n",
                "    def __invoke_api(self, url, query):\n",
                "        # Set the JSON payload data as a Python dictionary\n",
                "        json_payload = {\n",
                "            \"inputs\": query,\n",
                "            \"parameters\": {\n",
                "                \"max_new_tokens\": 1000,\n",
                "                \"temperature\": 0.1,\n",
                "                \"do_sample\": True\n",
                "            }\n",
                "        }\n",
                "\n",
                "        response = requests.post(\n",
                "            url, json=json_payload, verify=False, timeout=180)\n",
                "        print(response.json())\n",
                "        return response.json()['generated_text']\n",
                "\n",
                "    def __remove_query(self, answer, query):\n",
                "        return answer[len(query):].strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7cc32014-06cf-4c09-8390-8025942ef18e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_widget(model_name:str, placeholders:dict):\n",
                "    LLM_OPENAI = \"openai_text-davinci-003\"\n",
                "    LLM_MIXTRAL = \"Mixtral8X7B\"\n",
                "    if not model_name in [LLM_OPENAI, LLM_MIXTRAL]:\n",
                "        return \"Invalid model selected\"\n",
                "    qna_visualizer = QnAVisualizer(lhs_title='Prompt Template (LLM Request):', rhs_title='LLM Response:', \n",
                "                                   show_filter=False)\n",
                "    if model_name == LLM_OPENAI:\n",
                "        widget_title = \"Open AI (Closed Model)\"\n",
                "    elif model_name == LLM_MIXTRAL:\n",
                "        widget_title = \"Mixtral8X7B (Open Model)\"\n",
                "\n",
                "    def form_submit_button_clicked(_):\n",
                "        qna_visualizer.set_output_text('Fetching... Please wait...')\n",
                "        input_text = qna_visualizer.get_input_text()\n",
                "        \n",
                "        for key, value in placeholders.items():\n",
                "            key_str = \"{\" + key + \"}\"\n",
                "            input_text = input_text.replace(key_str, value)        \n",
                "        if model_name == LLM_OPENAI:\n",
                "            model_output = LlmService().call_open_ai(input_text)\n",
                "            print(model_output)\n",
                "        elif model_name == LLM_MIXTRAL:\n",
                "            model_output = LlmService().call_mixtral(input_text)\n",
                "        model_output = model_output.strip()\n",
                "        qna_visualizer.set_output_text(model_output)\n",
                "\n",
                "    # Set callback functions\n",
                "    qna_visualizer.set_token_counter_fn(count_tokens)\n",
                "    qna_visualizer.on_form_submit_callback(form_submit_button_clicked)\n",
                "    # Display UI\n",
                "    \n",
                "    display(HTML(f\"<h2>{widget_title}</h2>\"))\n",
                "    qna_visualizer.show_ui()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "09d4b219-7fe5-4ba2-8d66-07c622aa7d2e",
            "metadata": {},
            "outputs": [],
            "source": [
                "CONTEXT = \"\"\"\n",
                "In the depths of an abandoned train tunnel, John and Mark stumbled upon a mysterious machine. \n",
                "Its sleek, alien design hinted at a technology beyond their comprehension. With cautious excitement, \n",
                "they pressed buttons and pulled levers, inadvertently activating a time machine.\n",
                "\n",
                "The tunnel vanished, replaced by a bustling 19th-century street. Wide-eyed, they realized \n",
                "the power they held. They explored eras, witnessing history's highs and lows. From ancient Rome \n",
                "to a futuristic metropolis, time unfurled before them.\n",
                "\n",
                "But the machine's power waned. Panic set in as they realized they were trapped. \n",
                "They scrambled, trying to reverse their journey, but it was futile. The time machine \n",
                "blinked out, leaving them marooned in the past.\n",
                "\"\"\"\n",
                "\n",
                "QUESTION = \"What did John and Mark discover?\"\n",
                "\n",
                "placeholders = {\n",
                "    'context': CONTEXT,\n",
                "    'question': QUESTION\n",
                "    \n",
                "}\n",
                "SAMPLE_PROMPT_TEMPLATE_1 = \"\"\"\n",
                "Use the following pieces of context to answer the question at the end. \n",
                "If you don't know the answer or even doubtful a bit, just say that you don't know, \n",
                "don't try to make up an answer. Just give the shortest and most appropriate relevant answer \n",
                "to the question in proper json format with key as \"answer\". This json format should be followed \n",
                "even when answer is not found. \n",
                "{context}\n",
                "Question: {question}\n",
                "Helpful Answer:\n",
                "\"\"\"\n",
                "SAMPLE_PROMPT_TEMPLATE_2 = \"\"\"\n",
                "{context}\n",
                "Question: {question}\n",
                "Helpful Answer:\n",
                "\"\"\"\n",
                "print(\"---Sample prompt template 1---\", SAMPLE_PROMPT_TEMPLATE_1)\n",
                "print(\"---Sample prompt template 2---\", SAMPLE_PROMPT_TEMPLATE_2)\n",
                "show_widget(\"Mixtral8X7B\", placeholders)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "720de4ed-7cd3-4153-90a6-10549d574075",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "show_widget(\"openai_text-davinci-003\",placeholders)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}